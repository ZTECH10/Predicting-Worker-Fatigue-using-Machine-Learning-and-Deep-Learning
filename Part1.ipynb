{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Project summary \n",
    "\n",
    "Project title : Mental State Classification using EEG-based MUSE Brain-Machine Interface\n",
    "\n",
    "|Project Goal|:\n",
    "\n",
    "    -Create a machine learning model to predict the state of the brain between \"Normal\" and \"Fatigue\"\n",
    "\n",
    "|Use cases|:\n",
    "\n",
    "    -Use cases in high risk/dangerous jobs such as construction, pilots etc.\n",
    "\n",
    "|Data pre-preporcessing|:\n",
    "\n",
    "    -We tried to preprocess the data using mutiple methods to understand what gave us the best results.\n",
    "\n",
    "    -Generated 4 different datasets:\n",
    "    \n",
    "        data_preprocessing sub-folder: Here, we applied Gaussian based outlier detection and imputation of missing values on the original dataset.\n",
    "\n",
    "        data_preprocessing_with_PCA  sub-folder: Here, we added 5 PCAs to each dataset in the data_preprocessing sub-folder.\n",
    "\n",
    "        data_preprocessing_with_FastICA  sub-folder: Here, we added 5  FastICAs to each dataset in the data_preprocessing sub-folder.\n",
    "\n",
    "        data_preprocessing_with_FastICA_Temporal_Freq  sub-folder: Here, we  added 120 temporal-based features and 100 frequency-based features to each dataset in the data_preprocessing_with_FastICA  sub-folder.  The total number of features for each dataset in this sub-folder is now 245 features.\n",
    "\n",
    "\n",
    "|AutoML Library|: \n",
    "\n",
    "    -Used a library to help us define the best model to use and then created our model around that to save time and improve accuracy\n",
    "\n",
    "\n",
    "|Results|:\n",
    "\n",
    "    -Used the AutoML library to try different ML models on the 4 preprocessed datasets to determine which ML algorithm gives the highest accuracy. ML models considered: Baseline, Linear , Decision Tree, Random Forest, XGBoost, Neural Network.\n",
    "\n",
    "    -The highest accuracy was achieved on the data_preprocessing_with_PCA subfolder (99.1% with Neural Network)\n",
    "\n",
    "|Challenges|:\n",
    "\n",
    "    -Differentiating between fatigue and normal data​\n",
    "\n",
    "    -Misalignment of the Muse device (headband) leading to null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting the Temporal Raw Data to an Aggregated Data Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EoRP98MpR-qj"
   },
   "source": [
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N-qiINBQSK2g"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('datasets/Mohamed_Normal_1.csv')\n",
    "\n",
    "type(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing the Dataset in a Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  A Quick Review of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of rows in the dataset\n",
    "len(dataset.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# column data types\n",
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting  column 'Timestamp' to be of type datetime64 [ns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['TimeStamp'] = pd.to_datetime(dataset['TimeStamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type (dataset['TimeStamp'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drooping rows related to eye blinking or jaw clenching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the rows with at least 3 non-NA values and keep the DataFrame with valid entries in the same variable(i.e. dataset).\n",
    "dataset.dropna(thresh=3,axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretising time steps and creating an empty dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "time_step_size: time step size (aka granularity level) used to discretise time steps\n",
    "columns: The column labels of the DataFrame.\n",
    "index: The index (row labels) of the DataFrame.\n",
    "'''\n",
    "time_step_size= 100 \n",
    "unit='ms' # if unit changed, then change the unit in each encounter of 'timedelta(unit=time_step_size)'\n",
    "\n",
    "timestamps=pd.date_range(min(dataset['TimeStamp']), max(dataset['TimeStamp']), freq=str(time_step_size)+unit)\n",
    "\n",
    "\n",
    "cols =['Delta_TP9','Delta_AF7','Delta_AF8','Delta_TP10','Theta_TP9' ,'Theta_AF7','Theta_AF8','Theta_TP10','Alpha_TP9','Alpha_AF7','Alpha_AF8','Alpha_TP10','Beta_TP9','Beta_AF7','Beta_AF8','Beta_TP10','Gamma_TP9','Gamma_AF7','Gamma_AF8','Gamma_TP10','RAW_TP9','RAW_AF7','RAW_AF8'\n",
    ",'RAW_TP10']\n",
    "new_dataset= pd.DataFrame(index=timestamps, columns=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing the empty dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deriving the values of the empty dataset \n",
    "Now, we will derive the values for each  column at each discrete time step (i.e. each row of the new empty dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For row selection: common operation is the use of boolean vectors to filter the data.\n",
    "The operators are: | for or, & for and, and ~ for not. These must be grouped by using parentheses.\n",
    "source:Stackoverflow\n",
    "\n",
    "Each row in the new dataset represents a summary of the values encountered in the interval defined\n",
    "by the time step it was created for until the next time step, i.e. [t, t+ Δt).\"Chp2 of the book\"\n",
    "\n",
    "Aggreation type : average score\n",
    "'''\n",
    "for i in range(0, len(dataset.index)):\n",
    "    \n",
    "    # Row selection of the original dataset for each row in the new dataset\n",
    "    selected_rows = dataset[( dataset['TimeStamp'] >= new_dataset.index[i] ) & (dataset['TimeStamp'] < new_dataset.index[i] +  timedelta(milliseconds=time_step_size)) ]\n",
    "    \n",
    "    if len(selected_rows) >0:\n",
    "        print(len(selected_rows))\n",
    "        for col in cols:\n",
    "            new_dataset.loc[new_dataset.index[i],col]= np.average(selected_rows[col])\n",
    "        \n",
    "    else:\n",
    "        print(\"selected_rows was empty\")\n",
    "        for col in cols:\n",
    "            new_dataset.loc[new_dataset.index[i],col]= np.nan\n",
    "\n",
    "     \n",
    "     \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining Datasets \n",
    "\n",
    "Sources:\n",
    "\n",
    "[1] https://www.datacamp.com/community/tutorials/joining-dataframes-pandas\n",
    "\n",
    "[2] https://www.geeksforgeeks.org/how-to-drop-one-or-multiple-columns-in-pandas-dataframe/\n",
    "\n",
    "[3 https://machinelearningmastery.com/train-test-split-for-evaluating-machine-learning-algorithms/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets combination and mental state classficaiton "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "fileNames is a list with the names of the csv files within the 'datasets' path\n",
    "'''\n",
    "\n",
    "fileNames = []\n",
    "for file in os.listdir(\"datasets\"):\n",
    "    if file.endswith(\".csv\"):\n",
    "        fileNames.append(file)\n",
    "\n",
    "\n",
    "print(fileNames)\n",
    "print( 'Total number of csv files to be combined is : {}'.format(len(fileNames)))\n",
    "\n",
    "'''\n",
    "function that reads the file from the fileNames list, converts it to a dataFrame, and adds\n",
    "a mental state column to the dataframe: 0 for \"Fatigue\" and 1 for \"Normal\"\n",
    "'''\n",
    "def getFile(fn):\n",
    "    location = 'datasets/' + fn\n",
    "    df = pd.read_csv(location)\n",
    "    if \"Normal\" in fn:\n",
    "        df['State'] = 1\n",
    "    else:\n",
    "        # Fatigue-typed csv file\n",
    "        df['State'] = 0\n",
    "    return df\n",
    "\n",
    "\n",
    "'''\n",
    "List comprehension to create the final dataframe\n",
    "\n",
    "'''\n",
    "dfs = [getFile(file) for file in fileNames]\n",
    "df_final = pd.concat(dfs,ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing the Final Dataset in a Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Drooping rows related to eye blinking or jaw clenching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the rows with at least 4 non-NA values and keep the DataFrame with valid entries in the same variable(i.e. df_final).\n",
    "df_final.dropna(thresh=4,axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping other irrelvant cloums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all columns between column index 25 to 39\n",
    "df_final.drop(df_final.iloc[:, 25:39], inplace = True, axis = 1)\n",
    "#Remove the timestamp cloumn\n",
    "df_final.drop(['TimeStamp'], inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from Visualization import Visualization\n",
    "from OutlierDetection import DistributionBasedOutlierDetection\n",
    "from ImputationMissingValues import ImputationMissingValues\n",
    "from KalmanFilters import KalmanFilters\n",
    "\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isntances of our classes\n",
    "outlier_distr = DistributionBasedOutlierDetection()\n",
    "imputator=ImputationMissingValues()\n",
    "data_visualizer= Visualization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_folder='datasets'\n",
    "end_folder ='data_preprocessing'\n",
    "\n",
    "Path(end_folder).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "\n",
    "for file in os.scandir(start_folder):\n",
    "   \n",
    "        file_path = file.path\n",
    "        file_name=file.name\n",
    "        print(\"current file_path : \",file_path)\n",
    "        print(\"current file_name : \",file_name)\n",
    "        \n",
    "        if  file.name != '.DS_Store': \n",
    "            # index_col indicates what column of the csv to use as the indexes (row labels) of the dataframe\n",
    "            dataset = pd.read_csv(file_path,index_col=0)\n",
    "\n",
    "            \n",
    "\n",
    "            # Drooping rows related to eye blinking or jaw clenching by keeping only the rows with at least 3 non-NAN values \n",
    "            dataset.dropna(thresh=3,axis=0, inplace=True)\n",
    "\n",
    "            # Remove all columns between column index 20 to 39\n",
    "            dataset.drop(dataset.iloc[:, 20:39], inplace = True, axis = 1)\n",
    "\n",
    "\n",
    "            #Outlier detection using Guassian mixture model \n",
    "            for col in [col for col in dataset.columns if col != \"State\"]: \n",
    "                print('current column: {}'.format(col))\n",
    "                dataset = outlier_distr.mixture_model(dataset, col, 3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                print('Number of outliers with density threshold less than 0.0005 for column ' + col + ': ' + str(dataset[col+'_mixture'][dataset[col+'_mixture'] < 0.0005].count()))\n",
    "                dataset.loc[dataset['{}_mixture'.format(col)] < 0.0005, col] = np.nan # creating a missing value\n",
    "                del dataset[col + '_mixture']\n",
    "                \n",
    "                # imputating missing values \n",
    "                dataset= imputator.impute_interpolate(dataset,col)\n",
    "                print('NaNs total: {}'.format(dataset[col].isna().sum())  )\n",
    "               \n",
    "                \n",
    "\n",
    "\n",
    "            # Adding a mental state column to the dataset: 0 for \"Fatigue\" and 1 for \"Normal\"\n",
    "            if \"Normal\" in file_name:\n",
    "                dataset['State'] = 1\n",
    "            else:\n",
    "                # Fatigue-typed csv file\n",
    "                dataset['State'] = 0\n",
    "                \n",
    "            #Dataset visualization :\n",
    "            data_visualizer.plot_dataset(dataset, ['Gamma_','Beta_', 'Alpha_', 'Theta_', 'Delta_'],\n",
    "                    ['like', 'like', 'like', 'like', 'like', 'like'],\n",
    "                    ['line', 'line', 'line', 'line', 'line'],file_name.split('.')[0])\n",
    "            # save the new file in the new folder named end_folder\n",
    "            dataset.to_csv(end_folder +'/' + file.name)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engnieering  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataTransformation import PrincipalComponentAnalysis,IndependentComponentAnalysis\n",
    "\n",
    "from FrequencyAbstraction import FourierTransformation\n",
    "from TemporalAbstraction import TemporalNumericalAggregation\n",
    "\n",
    "PCA=PrincipalComponentAnalysis()\n",
    "FastICA=IndependentComponentAnalysis()\n",
    "\n",
    "# create an instance of the (FA) class\n",
    "\n",
    "freq_transformer = FourierTransformation()\n",
    "num_aggregator = TemporalNumericalAggregation()\n",
    "#create an instance of the (TA) class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_folder='data_preprocessing'\n",
    "end_folder ='data_preprocessing_with_PCA'\n",
    "\n",
    "Path(end_folder).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# find number_components for PCA\n",
    "\n",
    "\n",
    "# Finding explained variance ratios for each dataset\n",
    "PCA_all_evs=[]\n",
    "for file in os.scandir(start_folder):\n",
    "   \n",
    "        file_path = file.path\n",
    "        file_name=file.name\n",
    "        print(\"current file_path : \",file_path)\n",
    "        print(\"current file_name : \",file_name)\n",
    "        \n",
    "        if  file.name != '.DS_Store': \n",
    "            # index_col indicates what column of the csv to use as the indexes (row labels) of the dataframe\n",
    "            dataset = pd.read_csv(file_path,index_col=0)\n",
    "            cols=[ col for col in dataset.columns if col != \"State\" ]\n",
    "            PCA_ev = PCA.determine_pc_explained_variance(dataset, cols)\n",
    "            PCA_all_evs.append(PCA_ev)\n",
    "\n",
    "# Plotting explained variance ratios for each dataset\n",
    "\n",
    "for i in range (len(PCA_all_evs)):\n",
    "     data_visualizer.plot_xy(x=[range(1, len(cols)+1)], y=[PCA_all_evs[i]],\n",
    "                        xlabel='principal component number', ylabel='explained variance ratio',\n",
    "                        ylim=[0, 1], line_styles=['b-'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above 8 figures(i.e. 8 datasets), we can use the elbow method to find the optimal number of principal components:\n",
    "\n",
    "- Figure 1:   4\n",
    "- Figure 2:   3\n",
    "- Figure 3:   5\n",
    "- Figure 4:   5\n",
    "- Figure 5:   4\n",
    "- Figure 6:   5\n",
    "- Figure 7:   4\n",
    "- Figure 8:   4\n",
    "\n",
    "In the next part, We can use either 4 or 5 for the number_components variable since they are the two most common values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "number_components=5 \n",
    "\n",
    "\n",
    "# Loop through all the files\n",
    "for file in os.scandir(start_folder):\n",
    "   \n",
    "        file_path = file.path\n",
    "        file_name=file.name\n",
    "        print(\"current file_path : \",file_path)\n",
    "        print(\"current file_name : \",file_name)\n",
    "        \n",
    "        if  file.name != '.DS_Store': \n",
    "            # index_col indicates what column of the csv to use as the indexes (row labels) of the dataframe\n",
    "            dataset = pd.read_csv(file_path,index_col=0)\n",
    "            \n",
    "            # Applying PCA\n",
    "            cols=[ col for col in dataset.columns if col != \"State\" ]\n",
    "            dataset = PCA.apply_pca(copy.deepcopy(dataset),cols, number_components)\n",
    "            \n",
    "            # moving \"State\" column to the end\n",
    "            col_State_copy = dataset['State'].copy()\n",
    "            dataset=dataset.drop(['State'], axis = 1)\n",
    "            dataset['State']=col_State_copy\n",
    "            \n",
    "            #Dataset visualization :\n",
    "            data_visualizer.plot_dataset(dataset, ['Gamma_','Beta_', 'Alpha_', 'Theta_', 'Delta_','pca_'],\n",
    "                    ['like', 'like', 'like', 'like', 'like', 'like','like'],\n",
    "                    ['line', 'line', 'line', 'line', 'line','line'],file_name.split('.')[0])\n",
    "            # save the new file in the new folder named end_folder\n",
    "            dataset.to_csv(end_folder +'/' + file.name)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_folder='data_preprocessing'\n",
    "end_folder ='data_preprocessing_with_FastICA'\n",
    "Path(end_folder).mkdir(exist_ok=True, parents=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the same name of components for FastICA\n",
    "number_components_ICA=5\n",
    "# Loop through all the files\n",
    "for file in os.scandir(start_folder):\n",
    "   \n",
    "        file_path = file.path\n",
    "        file_name=file.name\n",
    "        print(\"current file_path : \",file_path)\n",
    "        print(\"current file_name : \",file_name)\n",
    "        \n",
    "        if  file.name != '.DS_Store': \n",
    "            # index_col indicates what column of the csv to use as the indexes (row labels) of the dataframe\n",
    "            dataset = pd.read_csv(file_path,index_col=0)\n",
    "            \n",
    "            # Applying Fast ICA\n",
    "            #https://stackoverflow.com/questions/27673231/why-should-i-make-a-copy-of-a-data-frame-in-pandas\n",
    "            cols=[ col for col in dataset.columns if col != \"State\" ]\n",
    "            dataset = FastICA.apply_fast_ica(copy.deepcopy(dataset),cols, number_components_ICA)\n",
    "            \n",
    "            # moving \"State\" column to the end\n",
    "            col_State_copy = dataset['State'].copy()\n",
    "            dataset=dataset.drop(['State'], axis = 1)\n",
    "            dataset['State']=col_State_copy\n",
    "            \n",
    "            #Dataset visualization :\n",
    "            data_visualizer.plot_dataset(dataset, ['Gamma_','Beta_', 'Alpha_', 'Theta_', 'Delta_','FastICA_'],\n",
    "                    ['like', 'like', 'like', 'like', 'like', 'like','like'],\n",
    "                    ['line', 'line', 'line', 'line', 'line','line'],file_name.split('.')[0])\n",
    "            # save the new file in the new folder named end_folder\n",
    "            dataset.to_csv(end_folder +'/' + file.name)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA & Temporal & Frequency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instances\n",
    "#freq_transformer\n",
    "# num_aggregator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_folder='data_preprocessing_with_FastICA'\n",
    "end_folder ='data_preprocessing_with_FastICA_Temporal_Freq'\n",
    "Path(end_folder).mkdir(exist_ok=True, parents=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all the files\n",
    "for file in os.scandir(start_folder):\n",
    "   \n",
    "        file_path = file.path\n",
    "        file_name=file.name\n",
    "        print(\"current file_path : \",file_path)\n",
    "        print(\"current file_name : \",file_name)\n",
    "        \n",
    "        if  file.name != '.DS_Store': \n",
    "            # index_col indicates what column of the csv to use as the indexes (row labels) of the dataframe\n",
    "            dataset = pd.read_csv(file_path,index_col=0)\n",
    "            dataset.index = pd.to_datetime(dataset.index)\n",
    "            #milliseconds_per_instance =((dataset.index[1] - dataset.index[0]).total_seconds() * 1000)\n",
    "            milliseconds_per_instance=1000\n",
    "           \n",
    "            \n",
    "            # window size of 2 sec for frequency and time domains \n",
    "            window_size =int(float(2000)/milliseconds_per_instance)\n",
    "            # Sampling rate\n",
    "            fs = float(200000)/milliseconds_per_instance # 200Hz\n",
    "            \n",
    "            # Applying temporal numerical aggregation\n",
    "            dataset = num_aggregator.abstract_numerical(dataset, cols, window_size, ['mean', 'median', 'max', 'min', 'std', 'slope'])\n",
    "            \n",
    "            #Applying fourier transformation \n",
    "            dataset = freq_transformer.abstract_frequency(dataset, cols,window_size, fs)\n",
    "            \n",
    "            \n",
    "            # An overlap of 50% \n",
    "            window_overlap = 0.5\n",
    "            skip_points = int((1-window_overlap) * window_size) ## if skip_points =1 , it means we will not skip any point\n",
    "            dataset = dataset.iloc[::skip_points,:]\n",
    "            \n",
    "            \n",
    "            # Moving \"State\" column to the end\n",
    "            col_State_copy = dataset['State'].copy()\n",
    "            dataset=dataset.drop(['State'], axis = 1)\n",
    "            dataset['State']=col_State_copy\n",
    "            \n",
    "            #Dataset visualization :\n",
    "            data_visualizer.plot_dataset(dataset, ['Gamma_','Beta_', 'Alpha_', 'Theta_', 'Delta_','FastICA_'],\n",
    "                    ['like', 'like', 'like', 'like', 'like', 'like','like'],\n",
    "                    ['line', 'line', 'line', 'line', 'line','line'],file_name.split('.')[0])\n",
    "            # save the new file in the new folder named end_folder\n",
    "            dataset.to_csv(end_folder +'/' + file.name)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data_preprocessing folder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "fileNames is a list with the names of the csv files within the specified \n",
    "folder path (i.e. data_preprocessing, data_preprocessing_with_PCA, data_preprocessing_with_FastICA etc.. )\n",
    "'''\n",
    "\n",
    "fileNames = []\n",
    "for file in os.listdir(\"data_preprocessing\"):\n",
    "    if file.endswith(\".csv\"):\n",
    "        fileNames.append(file)\n",
    "\n",
    "\n",
    "print(fileNames)\n",
    "print( 'Total number of csv files to be combined is : {}'.format(len(fileNames)))\n",
    "\n",
    "'''\n",
    "function that reads the file from the fileNames list and converts it to a dataFrame\n",
    "'''\n",
    "def getFile(fn):\n",
    "    location = 'data_preprocessing/' + fn\n",
    "    df = pd.read_csv(location)\n",
    "    return df\n",
    "\n",
    "\n",
    "'''\n",
    "List comprehension to create the final dataframe\n",
    "\n",
    "'''\n",
    "dfs = [getFile(file) for file in fileNames]\n",
    "df_final = pd.concat(dfs,ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Showing the Final Dataset in a Tabl\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['State'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final[df_final.columns[1:-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from supervised.automl import AutoML\n",
    "\n",
    "df= df_final\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[df.columns[1:-1]], df[\"State\"], test_size=0.25)\n",
    "\n",
    "automl = AutoML(eval_metric=\"accuracy\")\n",
    "automl.fit(X_train, y_train)\n",
    "\n",
    "predictions = automl.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data_preprocessing_with_PCA folder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "fileNames is a list with the names of the csv files within the specified \n",
    "folder path (i.e. data_preprocessing, data_preprocessing_with_PCA, data_preprocessing_with_FastICA etc.. )\n",
    "'''\n",
    "\n",
    "fileNames = []\n",
    "for file in os.listdir(\"data_preprocessing_with_PCA\"):\n",
    "    if file.endswith(\".csv\"):\n",
    "        fileNames.append(file)\n",
    "\n",
    "\n",
    "print(fileNames)\n",
    "print( 'Total number of csv files to be combined is : {}'.format(len(fileNames)))\n",
    "\n",
    "'''\n",
    "function that reads the file from the fileNames list and converts it to a dataFrame\n",
    "'''\n",
    "def getFile(fn):\n",
    "    location = 'data_preprocessing_with_PCA/' + fn\n",
    "    df = pd.read_csv(location)\n",
    "    return df\n",
    "\n",
    "\n",
    "'''\n",
    "List comprehension to create the final dataframe\n",
    "\n",
    "'''\n",
    "dfs = [getFile(file) for file in fileNames]\n",
    "df_final = pd.concat(dfs,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['State'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final[df_final.columns[1:-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from supervised.automl import AutoML\n",
    "\n",
    "df= df_final\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[df.columns[1:-1]], df[\"State\"], test_size=0.25)\n",
    "\n",
    "automl = AutoML(eval_metric=\"accuracy\")\n",
    "automl.fit(X_train, y_train)\n",
    "\n",
    "predictions = automl.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data_preprocessing_with_FastICA folder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "fileNames is a list with the names of the csv files within the specified \n",
    "folder path (i.e. data_preprocessing, data_preprocessing_with_PCA, data_preprocessing_with_FastICA etc.. )\n",
    "'''\n",
    "\n",
    "fileNames = []\n",
    "for file in os.listdir(\"data_preprocessing_with_FastICA\"):\n",
    "    if file.endswith(\".csv\"):\n",
    "        fileNames.append(file)\n",
    "\n",
    "\n",
    "print(fileNames)\n",
    "print( 'Total number of csv files to be combined is : {}'.format(len(fileNames)))\n",
    "\n",
    "'''\n",
    "function that reads the file from the fileNames list and converts it to a dataFrame\n",
    "'''\n",
    "def getFile(fn):\n",
    "    location = 'data_preprocessing_with_FastICA/' + fn\n",
    "    df = pd.read_csv(location)\n",
    "    return df\n",
    "\n",
    "\n",
    "'''\n",
    "List comprehension to create the final dataframe\n",
    "\n",
    "'''\n",
    "dfs = [getFile(file) for file in fileNames]\n",
    "df_final = pd.concat(dfs,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['State'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final[df_final.columns[1:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from supervised.automl import AutoML\n",
    "\n",
    "df= df_final\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[df.columns[1:-1]], df[\"State\"], test_size=0.25)\n",
    "\n",
    "automl = AutoML(eval_metric=\"accuracy\")\n",
    "automl.fit(X_train, y_train)\n",
    "\n",
    "predictions = automl.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data_preprocessing_with_FastICA_Temporal_Freq  folder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "fileNames is a list with the names of the csv files within the specified \n",
    "folder path (i.e. data_preprocessing, data_preprocessing_with_PCA, data_preprocessing_with_FastICA etc.. )\n",
    "'''\n",
    "\n",
    "fileNames = []\n",
    "for file in os.listdir(\"data_preprocessing_with_FastICA_Temporal_Freq\"):\n",
    "    if file.endswith(\".csv\"):\n",
    "        fileNames.append(file)\n",
    "\n",
    "\n",
    "print(fileNames)\n",
    "print( 'Total number of csv files to be combined is : {}'.format(len(fileNames)))\n",
    "\n",
    "'''\n",
    "function that reads the file from the fileNames list and converts it to a dataFrame\n",
    "'''\n",
    "def getFile(fn):\n",
    "    location = 'data_preprocessing_with_FastICA_Temporal_Freq/' + fn\n",
    "    df = pd.read_csv(location)\n",
    "    # Drooping rows with empty values; we will keep only the rows with at least 246 non-NAN values\n",
    "    # This step was added since Toma's orginal data data has a lot of zeros. Otherwise, we will choose \n",
    "    # 2 for \"tresh\" rather 246.\n",
    "    df.dropna(thresh=246,axis=0, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "'''\n",
    "List comprehension to create the final dataframe\n",
    "\n",
    "'''\n",
    "dfs = [getFile(file) for file in fileNames]\n",
    "df_final = pd.concat(dfs,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['State'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final[df_final.columns[1:-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from supervised.automl import AutoML\n",
    "\n",
    "df= df_final\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[df.columns[1:-1]], df[\"State\"], test_size=0.25)\n",
    "\n",
    "automl = AutoML(eval_metric=\"accuracy\")\n",
    "automl.fit(X_train, y_train)\n",
    "\n",
    "predictions = automl.predict(X_test)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "data_preprocessing_tools.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "289.006px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 357.5,
   "position": {
    "height": "379.5px",
    "left": "1161px",
    "right": "20px",
    "top": "122px",
    "width": "319px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
